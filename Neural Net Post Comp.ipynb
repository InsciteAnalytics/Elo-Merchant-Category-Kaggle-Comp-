{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Importing Libs & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T03:11:49.918184Z",
     "start_time": "2019-03-17T03:11:49.898876Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T18:06:53.046007Z",
     "start_time": "2019-03-16T18:06:52.895707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "XtrainLR = pd.read_pickle('XtrainLR');Ytrain = pd.read_pickle('Ytrain');XtestLR = pd.read_pickle('XtestLR')\n",
    "XvalLR = pd.read_pickle('XvalLR');Yval = pd.read_pickle('Yval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T02:35:30.271650Z",
     "start_time": "2019-03-17T02:35:29.513353Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "XtrainRF = pd.read_pickle('XtrainRF'); XtestRF = pd.read_pickle('XtestRF'); XvalRF = pd.read_pickle('XvalLR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fixing & exporting Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:28:17.610200Z",
     "start_time": "2019-03-16T20:28:16.473020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "XtrainLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]=preprocessing.scale(np.asarray(XtrainLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:28:17.872318Z",
     "start_time": "2019-03-16T20:28:17.614870Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "XvalLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]=preprocessing.scale(np.asarray(XvalLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T17:47:51.341768Z",
     "start_time": "2019-03-16T17:47:50.898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "XtestLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]=preprocessing.scale(np.asarray(XtestLR[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using Grid Search to check for best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:28:39.758008Z",
     "start_time": "2019-03-16T20:28:39.747180Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:31:56.986065Z",
     "start_time": "2019-03-12T17:31:56.964818Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=24, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:59:59.607607Z",
     "start_time": "2019-03-12T17:31:58.330601Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 10s 72us/step - loss: 2.8708 - acc: 0.0082\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8283 - acc: 0.0081\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8228 - acc: 0.0075\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8208 - acc: 0.0069\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8189 - acc: 0.0071\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8187 - acc: 0.0069\n",
      "Epoch 7/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8187 - acc: 0.0070\n",
      "Epoch 8/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8164 - acc: 0.0067\n",
      "Epoch 9/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8137 - acc: 0.0070\n",
      "Epoch 10/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8131 - acc: 0.0068\n",
      "Epoch 11/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8115 - acc: 0.0069\n",
      "Epoch 12/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8124 - acc: 0.0068\n",
      "Epoch 13/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.8093 - acc: 0.0066\n",
      "Epoch 14/100\n",
      "133070/133070 [==============================] - 11s 85us/step - loss: 2.8089 - acc: 0.0069\n",
      "Epoch 15/100\n",
      "133070/133070 [==============================] - 12s 87us/step - loss: 2.8080 - acc: 0.0071\n",
      "Epoch 16/100\n",
      "133070/133070 [==============================] - 9s 67us/step - loss: 2.8072 - acc: 0.0069\n",
      "Epoch 17/100\n",
      "133070/133070 [==============================] - 9s 67us/step - loss: 2.8061 - acc: 0.0068\n",
      "Epoch 18/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.8050 - acc: 0.0070\n",
      "Epoch 19/100\n",
      "133070/133070 [==============================] - 10s 74us/step - loss: 2.8045 - acc: 0.0070\n",
      "Epoch 20/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8034 - acc: 0.0071\n",
      "Epoch 21/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8053 - acc: 0.0067\n",
      "Epoch 22/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8037 - acc: 0.0069\n",
      "Epoch 23/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8010 - acc: 0.0068\n",
      "Epoch 24/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7989 - acc: 0.0072\n",
      "Epoch 25/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7982 - acc: 0.0067\n",
      "Epoch 26/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.7982 - acc: 0.0071\n",
      "Epoch 27/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7964 - acc: 0.0068\n",
      "Epoch 28/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.7984 - acc: 0.0069\n",
      "Epoch 29/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.7982 - acc: 0.0071\n",
      "Epoch 30/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.7940 - acc: 0.0069\n",
      "Epoch 31/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7927 - acc: 0.0068\n",
      "Epoch 32/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7903 - acc: 0.0067\n",
      "Epoch 33/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7905 - acc: 0.0069\n",
      "Epoch 34/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7891 - acc: 0.0069\n",
      "Epoch 35/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.7860 - acc: 0.0069\n",
      "Epoch 36/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7853 - acc: 0.0070\n",
      "Epoch 37/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7862 - acc: 0.0069\n",
      "Epoch 38/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7827 - acc: 0.0068\n",
      "Epoch 39/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7817 - acc: 0.0068\n",
      "Epoch 40/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7807 - acc: 0.0069\n",
      "Epoch 41/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7811 - acc: 0.0070\n",
      "Epoch 42/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7778 - acc: 0.0070\n",
      "Epoch 43/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7786 - acc: 0.0068\n",
      "Epoch 44/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7748 - acc: 0.0068\n",
      "Epoch 45/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7732 - acc: 0.0067\n",
      "Epoch 46/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7720 - acc: 0.0070\n",
      "Epoch 47/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7725 - acc: 0.0069\n",
      "Epoch 48/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7708 - acc: 0.0070\n",
      "Epoch 49/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7677 - acc: 0.0065\n",
      "Epoch 50/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7644 - acc: 0.0069\n",
      "Epoch 51/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7601 - acc: 0.0069\n",
      "Epoch 52/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.7669 - acc: 0.0070\n",
      "Epoch 53/100\n",
      "133070/133070 [==============================] - 13s 96us/step - loss: 2.7623 - acc: 0.0069\n",
      "Epoch 54/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.7572 - acc: 0.0066\n",
      "Epoch 55/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.7595 - acc: 0.0068\n",
      "Epoch 56/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7540 - acc: 0.0067\n",
      "Epoch 57/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7528 - acc: 0.0069\n",
      "Epoch 58/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7565 - acc: 0.0069\n",
      "Epoch 59/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7510 - acc: 0.0068\n",
      "Epoch 60/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7454 - acc: 0.0067\n",
      "Epoch 61/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7436 - acc: 0.0067\n",
      "Epoch 62/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7417 - acc: 0.0064\n",
      "Epoch 63/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7359 - acc: 0.0068\n",
      "Epoch 64/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7376 - acc: 0.0066\n",
      "Epoch 65/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7297 - acc: 0.0066\n",
      "Epoch 66/100\n",
      "133070/133070 [==============================] - 10s 75us/step - loss: 2.7283 - acc: 0.0066\n",
      "Epoch 67/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7282 - acc: 0.0065\n",
      "Epoch 68/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7257 - acc: 0.0065\n",
      "Epoch 69/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.7266 - acc: 0.0067\n",
      "Epoch 70/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7237 - acc: 0.0067\n",
      "Epoch 71/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7153 - acc: 0.0065\n",
      "Epoch 72/100\n",
      "133070/133070 [==============================] - 10s 72us/step - loss: 2.7198 - acc: 0.0066\n",
      "Epoch 73/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7093 - acc: 0.0065\n",
      "Epoch 74/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7106 - acc: 0.0065\n",
      "Epoch 75/100\n",
      "133070/133070 [==============================] - 15s 113us/step - loss: 2.7070 - acc: 0.0063\n",
      "Epoch 76/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.7085 - acc: 0.0062\n",
      "Epoch 77/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7015 - acc: 0.0064\n",
      "Epoch 78/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7004 - acc: 0.0067\n",
      "Epoch 79/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.6987 - acc: 0.0064\n",
      "Epoch 80/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.6862 - acc: 0.0063\n",
      "Epoch 81/100\n",
      "133070/133070 [==============================] - 10s 77us/step - loss: 2.6826 - acc: 0.0065\n",
      "Epoch 82/100\n",
      "133070/133070 [==============================] - 11s 84us/step - loss: 2.6832 - acc: 0.0062\n",
      "Epoch 83/100\n",
      "133070/133070 [==============================] - 10s 77us/step - loss: 2.6734 - acc: 0.0063\n",
      "Epoch 84/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.6703 - acc: 0.0063\n",
      "Epoch 85/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6721 - acc: 0.0061\n",
      "Epoch 86/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6640 - acc: 0.0061\n",
      "Epoch 87/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6549 - acc: 0.0062\n",
      "Epoch 88/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6518 - acc: 0.0062\n",
      "Epoch 89/100\n",
      "133070/133070 [==============================] - 10s 76us/step - loss: 2.6514 - acc: 0.0059\n",
      "Epoch 90/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6428 - acc: 0.0061\n",
      "Epoch 91/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6503 - acc: 0.0061\n",
      "Epoch 92/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.6376 - acc: 0.0061\n",
      "Epoch 93/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6370 - acc: 0.0062\n",
      "Epoch 94/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6302 - acc: 0.0061\n",
      "Epoch 95/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6224 - acc: 0.0062\n",
      "Epoch 96/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.6174 - acc: 0.0060\n",
      "Epoch 97/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6173 - acc: 0.0060\n",
      "Epoch 98/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6139 - acc: 0.0060\n",
      "Epoch 99/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6056 - acc: 0.0061\n",
      "Epoch 100/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6090 - acc: 0.0060\n",
      "66535/66535 [==============================] - ETA:  - 2s 26us/step\n",
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.9220 - acc: 0.0080\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8705 - acc: 0.0081\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8663 - acc: 0.0081\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8634 - acc: 0.0080\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8611 - acc: 0.0075\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8588 - acc: 0.0071\n",
      "Epoch 7/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8579 - acc: 0.0071\n",
      "Epoch 8/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8556 - acc: 0.0068\n",
      "Epoch 9/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8545 - acc: 0.0067\n",
      "Epoch 10/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8550 - acc: 0.0067\n",
      "Epoch 11/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8532 - acc: 0.0066\n",
      "Epoch 12/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8514 - acc: 0.0066\n",
      "Epoch 13/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8511 - acc: 0.0066\n",
      "Epoch 14/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8491 - acc: 0.0066\n",
      "Epoch 15/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8469 - acc: 0.0067\n",
      "Epoch 16/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8476 - acc: 0.0066\n",
      "Epoch 17/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8474 - acc: 0.0067\n",
      "Epoch 18/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8454 - acc: 0.0065\n",
      "Epoch 19/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8437 - acc: 0.0066\n",
      "Epoch 20/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8423 - acc: 0.0069\n",
      "Epoch 21/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8448 - acc: 0.0068\n",
      "Epoch 22/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8416 - acc: 0.0068\n",
      "Epoch 23/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8390 - acc: 0.0067\n",
      "Epoch 24/100\n",
      "133070/133070 [==============================] - 9s 65us/step - loss: 2.8407 - acc: 0.0068\n",
      "Epoch 25/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8398 - acc: 0.0066\n",
      "Epoch 26/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8383 - acc: 0.0067\n",
      "Epoch 27/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8380 - acc: 0.0068\n",
      "Epoch 28/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8359 - acc: 0.0066\n",
      "Epoch 29/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8355 - acc: 0.0069\n",
      "Epoch 30/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8336 - acc: 0.0067\n",
      "Epoch 31/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8320 - acc: 0.0070\n",
      "Epoch 32/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8366 - acc: 0.0068\n",
      "Epoch 33/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8309 - acc: 0.0069\n",
      "Epoch 34/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8289 - acc: 0.0070\n",
      "Epoch 35/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8316 - acc: 0.0065\n",
      "Epoch 36/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8272 - acc: 0.0067\n",
      "Epoch 37/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8259 - acc: 0.0069\n",
      "Epoch 38/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8233 - acc: 0.0067\n",
      "Epoch 39/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8221 - acc: 0.0067\n",
      "Epoch 40/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8225 - acc: 0.0068\n",
      "Epoch 41/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8191 - acc: 0.0066\n",
      "Epoch 42/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8182 - acc: 0.0064\n",
      "Epoch 43/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8195 - acc: 0.0069\n",
      "Epoch 44/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8185 - acc: 0.0066\n",
      "Epoch 45/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8156 - acc: 0.0068\n",
      "Epoch 46/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8116 - acc: 0.0066\n",
      "Epoch 47/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8119 - acc: 0.0067\n",
      "Epoch 48/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8106 - acc: 0.0067\n",
      "Epoch 49/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8074 - acc: 0.0066\n",
      "Epoch 50/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8079 - acc: 0.0067\n",
      "Epoch 51/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8065 - acc: 0.0067\n",
      "Epoch 52/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8038 - acc: 0.0066\n",
      "Epoch 53/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7989 - acc: 0.0067\n",
      "Epoch 54/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7962 - acc: 0.0065\n",
      "Epoch 55/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7936 - acc: 0.0064\n",
      "Epoch 56/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7913 - acc: 0.0065\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7944 - acc: 0.0068\n",
      "Epoch 58/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7874 - acc: 0.0062\n",
      "Epoch 59/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7820 - acc: 0.0066\n",
      "Epoch 60/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7862 - acc: 0.0066\n",
      "Epoch 61/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7897 - acc: 0.0063\n",
      "Epoch 62/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7846 - acc: 0.0064\n",
      "Epoch 63/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7754 - acc: 0.0066\n",
      "Epoch 64/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7744 - acc: 0.0064\n",
      "Epoch 65/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7734 - acc: 0.0061\n",
      "Epoch 66/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7699 - acc: 0.0063\n",
      "Epoch 67/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7670 - acc: 0.0063\n",
      "Epoch 68/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7681 - acc: 0.0063\n",
      "Epoch 69/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7669 - acc: 0.0062\n",
      "Epoch 70/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7604 - acc: 0.0063\n",
      "Epoch 71/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.7535 - acc: 0.0062\n",
      "Epoch 72/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7547 - acc: 0.0064\n",
      "Epoch 73/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7473 - acc: 0.0061\n",
      "Epoch 74/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7468 - acc: 0.0062\n",
      "Epoch 75/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7458 - acc: 0.0059\n",
      "Epoch 76/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7344 - acc: 0.0062\n",
      "Epoch 77/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7303 - acc: 0.0059\n",
      "Epoch 78/100\n",
      "133070/133070 [==============================] - 7s 56us/step - loss: 2.7339 - acc: 0.0060\n",
      "Epoch 79/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7255 - acc: 0.0062\n",
      "Epoch 80/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7168 - acc: 0.0063\n",
      "Epoch 81/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7128 - acc: 0.0059\n",
      "Epoch 82/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7163 - acc: 0.0061\n",
      "Epoch 83/100\n",
      "133070/133070 [==============================] - 7s 56us/step - loss: 2.7053 - acc: 0.0061\n",
      "Epoch 84/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7038 - acc: 0.0061\n",
      "Epoch 85/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6953 - acc: 0.0061\n",
      "Epoch 86/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7009 - acc: 0.0060\n",
      "Epoch 87/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6938 - acc: 0.0059\n",
      "Epoch 88/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.6861 - acc: 0.0062\n",
      "Epoch 89/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6903 - acc: 0.0060\n",
      "Epoch 90/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6878 - acc: 0.0057\n",
      "Epoch 91/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6828 - acc: 0.0059\n",
      "Epoch 92/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6734 - acc: 0.0061\n",
      "Epoch 93/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6573 - acc: 0.0060\n",
      "Epoch 94/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6581 - acc: 0.0058\n",
      "Epoch 95/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6653 - acc: 0.0058\n",
      "Epoch 96/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6832 - acc: 0.0058\n",
      "Epoch 97/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6634 - acc: 0.0059\n",
      "Epoch 98/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6569 - acc: 0.0059\n",
      "Epoch 99/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6545 - acc: 0.0059\n",
      "Epoch 100/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6425 - acc: 0.0058\n",
      "66535/66535 [==============================] - 2s 26us/step\n",
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 9s 70us/step - loss: 2.9118 - acc: 0.0081\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.8506 - acc: 0.0082\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8446 - acc: 0.0081\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.8417 - acc: 0.0076\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8403 - acc: 0.0074\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8382 - acc: 0.0073\n",
      "Epoch 7/100\n",
      "100000/133070 [=====================>........] - ETA: 2s - loss: 2.8252 - acc: 0.0069"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-7288a762c50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: %.2f (%.2f) MSE\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "results = cross_val_score(estimator, Xtrain, Ytrain, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fitting best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T20:29:56.850237Z",
     "start_time": "2019-03-16T20:29:56.845848Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T23:34:42.535642Z",
     "start_time": "2019-03-16T23:34:42.520019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reduceLR = ReduceLROnPlateau(monitor='loss',factor=0.1,verbose=1,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T01:43:39.849034Z",
     "start_time": "2019-03-17T01:02:46.198573Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "159684/159684 [==============================] - 17s 107us/step - loss: 5.0194\n",
      "Epoch 2/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 4.6614\n",
      "Epoch 3/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 4.3446\n",
      "Epoch 4/250\n",
      "159684/159684 [==============================] - 18s 113us/step - loss: 4.0619\n",
      "Epoch 5/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 3.8190\n",
      "Epoch 6/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 3.6140\n",
      "Epoch 7/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 3.4467\n",
      "Epoch 8/250\n",
      "159684/159684 [==============================] - 16s 102us/step - loss: 3.3122\n",
      "Epoch 9/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 3.2062\n",
      "Epoch 10/250\n",
      "159684/159684 [==============================] - 16s 100us/step - loss: 3.1215\n",
      "Epoch 11/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 3.0572\n",
      "Epoch 12/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 3.0051\n",
      "Epoch 13/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.9686\n",
      "Epoch 14/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.9379\n",
      "Epoch 15/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.9162\n",
      "Epoch 16/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8991\n",
      "Epoch 17/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8874\n",
      "Epoch 18/250\n",
      "159684/159684 [==============================] - 16s 100us/step - loss: 2.8774\n",
      "Epoch 19/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8695\n",
      "Epoch 20/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8639\n",
      "Epoch 21/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8600\n",
      "Epoch 22/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8567\n",
      "Epoch 23/250\n",
      "159684/159684 [==============================] - 16s 103us/step - loss: 2.8541\n",
      "Epoch 24/250\n",
      "159684/159684 [==============================] - 16s 102us/step - loss: 2.8516\n",
      "Epoch 25/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8517\n",
      "Epoch 26/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8504\n",
      "Epoch 27/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8485\n",
      "Epoch 28/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8478\n",
      "Epoch 29/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8461\n",
      "Epoch 30/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8478\n",
      "Epoch 31/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8469\n",
      "Epoch 32/250\n",
      "159684/159684 [==============================] - 16s 97us/step - loss: 2.8465\n",
      "Epoch 33/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8450\n",
      "Epoch 34/250\n",
      "159684/159684 [==============================] - 16s 100us/step - loss: 2.8453\n",
      "Epoch 35/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8460\n",
      "Epoch 36/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8440\n",
      "Epoch 37/250\n",
      "159684/159684 [==============================] - 16s 97us/step - loss: 2.8440\n",
      "Epoch 38/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8430\n",
      "Epoch 39/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8429\n",
      "Epoch 40/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8430\n",
      "Epoch 41/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8408\n",
      "Epoch 42/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8431\n",
      "Epoch 43/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8429\n",
      "Epoch 44/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8423\n",
      "Epoch 45/250\n",
      "159684/159684 [==============================] - 16s 100us/step - loss: 2.8416\n",
      "Epoch 46/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8398\n",
      "Epoch 47/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8410\n",
      "Epoch 48/250\n",
      "159684/159684 [==============================] - 14s 91us/step - loss: 2.8412\n",
      "Epoch 49/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8407\n",
      "Epoch 50/250\n",
      "159684/159684 [==============================] - 17s 104us/step - loss: 2.8408\n",
      "Epoch 51/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8404\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 52/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8401\n",
      "Epoch 53/250\n",
      "159684/159684 [==============================] - 15s 92us/step - loss: 2.8392\n",
      "Epoch 54/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8379\n",
      "Epoch 55/250\n",
      "159684/159684 [==============================] - 18s 110us/step - loss: 2.8380\n",
      "Epoch 56/250\n",
      "159684/159684 [==============================] - 16s 97us/step - loss: 2.8370\n",
      "Epoch 57/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8360\n",
      "Epoch 58/250\n",
      "159684/159684 [==============================] - 16s 97us/step - loss: 2.8353\n",
      "Epoch 59/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8347\n",
      "Epoch 60/250\n",
      "159684/159684 [==============================] - 16s 101us/step - loss: 2.8335\n",
      "Epoch 61/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8352\n",
      "Epoch 62/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8338\n",
      "Epoch 63/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8341\n",
      "Epoch 64/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8338\n",
      "Epoch 65/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8335\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 66/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8337\n",
      "Epoch 67/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8325\n",
      "Epoch 68/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8333\n",
      "Epoch 69/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8329\n",
      "Epoch 70/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8326\n",
      "Epoch 71/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8329\n",
      "Epoch 72/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8320\n",
      "Epoch 73/250\n",
      "159684/159684 [==============================] - 16s 99us/step - loss: 2.8327\n",
      "Epoch 74/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8327\n",
      "Epoch 75/250\n",
      "159684/159684 [==============================] - 15s 95us/step - loss: 2.8327\n",
      "Epoch 76/250\n",
      "159684/159684 [==============================] - 17s 108us/step - loss: 2.8329\n",
      "Epoch 77/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8329\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 78/250\n",
      "159684/159684 [==============================] - 15s 97us/step - loss: 2.8322\n",
      "Epoch 79/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8323\n",
      "Epoch 80/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8325\n",
      "Epoch 81/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8326\n",
      "Epoch 82/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8324\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 83/250\n",
      "159684/159684 [==============================] - 14s 91us/step - loss: 2.8322\n",
      "Epoch 84/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8318\n",
      "Epoch 85/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8319\n",
      "Epoch 86/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8327\n",
      "Epoch 87/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8328\n",
      "Epoch 88/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8328\n",
      "Epoch 89/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8322\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 90/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8324\n",
      "Epoch 91/250\n",
      "159684/159684 [==============================] - 15s 92us/step - loss: 2.8324\n",
      "Epoch 92/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8325\n",
      "Epoch 93/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8328\n",
      "Epoch 94/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8328\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 95/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8334\n",
      "Epoch 96/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8324\n",
      "Epoch 97/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8320\n",
      "Epoch 98/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8315\n",
      "Epoch 99/250\n",
      "159684/159684 [==============================] - 17s 106us/step - loss: 2.8330\n",
      "Epoch 100/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8332\n",
      "Epoch 101/250\n",
      "159684/159684 [==============================] - 15s 91us/step - loss: 2.8315\n",
      "Epoch 102/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8331\n",
      "Epoch 103/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8329\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 104/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8324\n",
      "Epoch 105/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8333\n",
      "Epoch 106/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8322\n",
      "Epoch 107/250\n",
      "159684/159684 [==============================] - 15s 93us/step - loss: 2.8325\n",
      "Epoch 108/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8322\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 109/250\n",
      "159684/159684 [==============================] - 15s 92us/step - loss: 2.8330\n",
      "Epoch 110/250\n",
      "159684/159684 [==============================] - 17s 105us/step - loss: 2.8325\n",
      "Epoch 111/250\n",
      "159684/159684 [==============================] - 15s 96us/step - loss: 2.8332\n",
      "Epoch 112/250\n",
      "159684/159684 [==============================] - 15s 94us/step - loss: 2.8328\n",
      "Epoch 113/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8318\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 114/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8332\n",
      "Epoch 115/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8326\n",
      "Epoch 116/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8325\n",
      "Epoch 117/250\n",
      "159684/159684 [==============================] - 15s 92us/step - loss: 2.8321\n",
      "Epoch 118/250\n",
      "159684/159684 [==============================] - 16s 97us/step - loss: 2.8319\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Epoch 119/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8320\n",
      "Epoch 120/250\n",
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8330\n",
      "Epoch 121/250\n",
      "159684/159684 [==============================] - 13s 84us/step - loss: 2.8316\n",
      "Epoch 122/250\n",
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8322\n",
      "Epoch 123/250\n",
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8325\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "Epoch 124/250\n",
      "159684/159684 [==============================] - 13s 81us/step - loss: 2.8325\n",
      "Epoch 125/250\n",
      "159684/159684 [==============================] - 12s 77us/step - loss: 2.8332\n",
      "Epoch 126/250\n",
      "159684/159684 [==============================] - 13s 81us/step - loss: 2.8316\n",
      "Epoch 127/250\n",
      "159684/159684 [==============================] - 13s 79us/step - loss: 2.8332\n",
      "Epoch 128/250\n",
      "159684/159684 [==============================] - 15s 91us/step - loss: 2.8323\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "Epoch 129/250\n",
      "159684/159684 [==============================] - 18s 113us/step - loss: 2.8328\n",
      "Epoch 130/250\n",
      "159684/159684 [==============================] - 16s 102us/step - loss: 2.8328\n",
      "Epoch 131/250\n",
      "159684/159684 [==============================] - 14s 90us/step - loss: 2.8321\n",
      "Epoch 132/250\n",
      "159684/159684 [==============================] - 16s 98us/step - loss: 2.8327\n",
      "Epoch 133/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8326\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "Epoch 134/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8324\n",
      "Epoch 135/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8338\n",
      "Epoch 136/250\n",
      "159684/159684 [==============================] - 14s 89us/step - loss: 2.8320\n",
      "Epoch 137/250\n",
      "159684/159684 [==============================] - 13s 84us/step - loss: 2.8322\n",
      "Epoch 138/250\n",
      "159684/159684 [==============================] - 14s 85us/step - loss: 2.8322\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "Epoch 139/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8330\n",
      "Epoch 140/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8327\n",
      "Epoch 141/250\n",
      "159684/159684 [==============================] - 13s 84us/step - loss: 2.8315\n",
      "Epoch 142/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8333\n",
      "Epoch 143/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8328\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "Epoch 144/250\n",
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8324\n",
      "Epoch 145/250\n",
      "159684/159684 [==============================] - 13s 84us/step - loss: 2.8318\n",
      "Epoch 146/250\n",
      "159684/159684 [==============================] - 14s 86us/step - loss: 2.8317\n",
      "Epoch 147/250\n",
      "159684/159684 [==============================] - 13s 84us/step - loss: 2.8320\n",
      "Epoch 148/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8326\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "Epoch 149/250\n",
      "159684/159684 [==============================] - 12s 78us/step - loss: 2.8317\n",
      "Epoch 150/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8318\n",
      "Epoch 151/250\n",
      "159684/159684 [==============================] - 13s 79us/step - loss: 2.8320\n",
      "Epoch 152/250\n",
      "159684/159684 [==============================] - 13s 80us/step - loss: 2.8335\n",
      "Epoch 153/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8327\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "Epoch 154/250\n",
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8323\n",
      "Epoch 155/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8327\n",
      "Epoch 156/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8330\n",
      "Epoch 157/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8325\n",
      "Epoch 158/250\n",
      "159684/159684 [==============================] - 13s 79us/step - loss: 2.8317\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "Epoch 159/250\n",
      "159684/159684 [==============================] - 13s 83us/step - loss: 2.8319\n",
      "Epoch 160/250\n",
      "159684/159684 [==============================] - 13s 82us/step - loss: 2.8334\n",
      "Epoch 161/250\n",
      "159684/159684 [==============================] - 14s 88us/step - loss: 2.8327\n",
      "Epoch 162/250\n",
      "159684/159684 [==============================] - 14s 85us/step - loss: 2.8327\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159684/159684 [==============================] - 14s 87us/step - loss: 2.8319\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "Epoch 164/250\n",
      "159684/159684 [==============================] - 14s 86us/step - loss: 2.8324\n",
      "Epoch 165/250\n",
      "159684/159684 [==============================] - 14s 86us/step - loss: 2.8317\n",
      "Epoch 166/250\n",
      "159684/159684 [==============================] - 14s 85us/step - loss: 2.8329\n",
      "Epoch 167/250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-908d34b4504b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 3. fit the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrainLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreduceLR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=24, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(400, activation='relu',kernel_regularizer=l2(0.005)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(75, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# 2. compile the network\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# 3. fit the network\n",
    "history = model.fit(XtrainLR, Ytrain, epochs=250, batch_size=30000, callbacks=[reduceLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T01:51:57.059812Z",
     "start_time": "2019-03-17T01:51:32.831068Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159684/159684 [==============================] - 19s 122us/step\n",
      "39921/39921 [==============================] - 5s 116us/step\n",
      "Training RMSE: 1.6822908992183263\n",
      "Validation RMSE: 1.687576354278096\n"
     ]
    }
   ],
   "source": [
    "# 4. evaluate the network\n",
    "losstrain = model.evaluate(XtrainLR,Ytrain)\n",
    "lossval = model.evaluate(XvalLR, Yval)\n",
    "print('Training RMSE:',sqrt(losstrain))\n",
    "print('Validation RMSE:',sqrt(lossval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T00:06:45.334110Z",
     "start_time": "2019-03-17T00:06:45.324018Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_92 (Dense)             (None, 500)               12500     \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 400)               200400    \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 75)                7575      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 50)                3800      \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 451,586\n",
      "Trainable params: 451,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:27:33.145970Z",
     "start_time": "2019-03-13T03:27:26.350698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5. make predictions\n",
    "Ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:27:33.630235Z",
     "start_time": "2019-03-13T03:27:33.145970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targets = pd.DataFrame(Ypred,columns=['target']); targets.to_csv('targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:51:46.723081Z",
     "start_time": "2019-03-12T21:51:46.691836Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.146940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.440055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.958091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.107839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.687379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.412674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.264124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.073686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.511530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.445657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.645471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.434187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.060336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.224298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.223935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.245104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.106159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.051506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.611010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.312891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.130725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.812803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.349580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.467421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.304362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.016946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123593</th>\n",
       "      <td>-0.238736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123594</th>\n",
       "      <td>-0.064388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123595</th>\n",
       "      <td>0.139949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123596</th>\n",
       "      <td>-0.328311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123597</th>\n",
       "      <td>0.174461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123598</th>\n",
       "      <td>-0.097019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123599</th>\n",
       "      <td>-0.816208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123600</th>\n",
       "      <td>-0.773228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123601</th>\n",
       "      <td>0.045559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123602</th>\n",
       "      <td>-0.853169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123603</th>\n",
       "      <td>-0.325822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123604</th>\n",
       "      <td>-0.543833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123605</th>\n",
       "      <td>0.002259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123606</th>\n",
       "      <td>-0.535410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123607</th>\n",
       "      <td>0.727111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123608</th>\n",
       "      <td>-0.201082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123609</th>\n",
       "      <td>0.117222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123610</th>\n",
       "      <td>-0.800819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123611</th>\n",
       "      <td>0.240039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123612</th>\n",
       "      <td>-0.130933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123613</th>\n",
       "      <td>0.379224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123614</th>\n",
       "      <td>-1.597571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123615</th>\n",
       "      <td>-0.391914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123616</th>\n",
       "      <td>-0.026032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123617</th>\n",
       "      <td>-0.152784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123618</th>\n",
       "      <td>-0.148802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123619</th>\n",
       "      <td>0.341019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123620</th>\n",
       "      <td>-1.146644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123621</th>\n",
       "      <td>-0.660543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123622</th>\n",
       "      <td>-0.024831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123623 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "0       0.077040\n",
       "1       0.385990\n",
       "2       0.146940\n",
       "3      -0.440055\n",
       "4      -0.958091\n",
       "5      -0.107839\n",
       "6      -0.687379\n",
       "7       0.412674\n",
       "8       0.264124\n",
       "9       0.058995\n",
       "10     -0.002074\n",
       "11      0.073686\n",
       "12     -0.511530\n",
       "13     -0.445657\n",
       "14     -0.645471\n",
       "15     -0.434187\n",
       "16     -0.060336\n",
       "17      0.224298\n",
       "18      0.223935\n",
       "19      0.245104\n",
       "20      0.106159\n",
       "21     -0.051506\n",
       "22     -0.611010\n",
       "23      0.312891\n",
       "24      0.130725\n",
       "25      0.812803\n",
       "26     -0.349580\n",
       "27      0.467421\n",
       "28     -0.304362\n",
       "29      0.016946\n",
       "...          ...\n",
       "123593 -0.238736\n",
       "123594 -0.064388\n",
       "123595  0.139949\n",
       "123596 -0.328311\n",
       "123597  0.174461\n",
       "123598 -0.097019\n",
       "123599 -0.816208\n",
       "123600 -0.773228\n",
       "123601  0.045559\n",
       "123602 -0.853169\n",
       "123603 -0.325822\n",
       "123604 -0.543833\n",
       "123605  0.002259\n",
       "123606 -0.535410\n",
       "123607  0.727111\n",
       "123608 -0.201082\n",
       "123609  0.117222\n",
       "123610 -0.800819\n",
       "123611  0.240039\n",
       "123612 -0.130933\n",
       "123613  0.379224\n",
       "123614 -1.597571\n",
       "123615 -0.391914\n",
       "123616 -0.026032\n",
       "123617 -0.152784\n",
       "123618 -0.148802\n",
       "123619  0.341019\n",
       "123620 -1.146644\n",
       "123621 -0.660543\n",
       "123622 -0.024831\n",
       "\n",
       "[123623 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T18:54:53.256142Z",
     "start_time": "2019-03-11T18:54:48.015129Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = [float(round(x)) for x in probabilities]\n",
    "# accuracy = numpy.mean(predictions == Y)\n",
    "# print(\"Prediction Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making feature dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was an experiment done using just the 3 features present in the original training set. I ran a Neural Net on just those features to check if predictability was better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T15:37:23.916232Z",
     "start_time": "2019-03-13T15:37:23.853864Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xtrain2 = Xtrain[['feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1']]\n",
    "Xtest2 = Xtest[['feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### NN on features only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T16:22:16.409351Z",
     "start_time": "2019-03-13T16:22:16.393698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=7, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# 2. compile the network\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['accuracy'])\n",
    "# 3. fit the network\n",
    "history = model.fit(Xtrain2, Ytrain, epochs=50, batch_size=15000)\n",
    "# 4. evaluate the network\n",
    "loss, accuracy = model.evaluate(Xtrain2, Ytrain)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Applying Random Forest on features only dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Importing fresh data, fixing issues again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T02:59:48.217691Z",
     "start_time": "2019-03-17T02:59:48.036106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Train = pd.read_pickle('NewTrain2') \n",
    "\n",
    "Train = Train[['first_active_month','feature_1','feature_2','feature_3','target']]\n",
    "\n",
    "Xtrain = Train.loc[:,Train.columns!='target']; Ytrain = Train.loc[:,'target']\n",
    "\n",
    "Xtrain.first_active_month = Xtrain.first_active_month.astype('category')\n",
    "\n",
    "Xtrain,Xval,Ytrain,Yval=train_test_split(Xtrain,Ytrain,test_size=0.2)\n",
    "\n",
    "Xtrain.first_active_month = Xtrain.first_active_month.cat.codes\n",
    "\n",
    "Xtrain.drop(['first_active_month'],axis=1,inplace=True); Xval.drop(['first_active_month'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Running RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T04:11:55.110450Z",
     "start_time": "2019-03-17T04:11:44.652840Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_estimators=500, n_jobs=-1, min_samples_leaf=3, max_features=0.5)\n",
    "model = RF.fit(Xtrain, Ytrain)\n",
    "\n",
    "Ypred = RF.predict(Xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T04:11:55.910577Z",
     "start_time": "2019-03-17T04:11:55.878718Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.686474445851263"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE 1st Check\n",
    "MSE=mean_squared_error(Yval,Ypred)\n",
    "RMSE=sqrt(MSE);RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T04:12:28.492790Z",
     "start_time": "2019-03-17T04:12:28.383546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Test = pd.read_pickle('NewTest2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T03:57:56.796128Z",
     "start_time": "2019-03-17T03:57:56.328688Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_2    0.474581\n",
       "feature_1    0.449307\n",
       "feature_3    0.076112\n",
       "dtype: float64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FI = pd.Series(RF.feature_importances_,index=Xtrain.columns).sort_values(ascending=False);FI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

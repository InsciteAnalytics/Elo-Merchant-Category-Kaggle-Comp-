{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing & exporting Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:16:32.487594Z",
     "start_time": "2019-03-12T15:16:31.206583Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]=preprocessing.scale(np.asarray(Xtrain[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:30:27.786477Z",
     "start_time": "2019-03-12T15:30:27.286747Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtest[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]=preprocessing.scale(np.asarray(Xtest[['purchase_amount', 'Cat_1_Y',\\\n",
    "       'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3', 'Cat2Tot4', 'Cat3TotA', 'Cat3TotB',\\\n",
    "       'AvgPurAmt', 'TimeSpent', 'CLV', 'numerical_2', 'avg_sales_lag3',\\\n",
    "       'avg_purchases_lag3', 'avg_purchases_lag6', 'active_months_lag6',\\\n",
    "       'avg_sales_lag12']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:19:32.221440Z",
     "start_time": "2019-03-12T15:19:32.158859Z"
    }
   },
   "outputs": [],
   "source": [
    "XtrainNN = pd.get_dummies(Xtrain,columns=['feature_1','feature_2','feature_3'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:30:45.896844Z",
     "start_time": "2019-03-12T15:30:45.803030Z"
    }
   },
   "outputs": [],
   "source": [
    "XtestNN = pd.get_dummies(Xtest,columns=['feature_1','feature_2','feature_3'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:31:47.867045Z",
     "start_time": "2019-03-12T15:31:47.335674Z"
    }
   },
   "outputs": [],
   "source": [
    "XtrainNN.to_pickle('XtrainNN');XtestNN.to_pickle('XtestNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Importing Libs & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T16:41:27.901183Z",
     "start_time": "2019-03-13T16:41:27.885527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.regularizers import l1\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T15:36:03.337148Z",
     "start_time": "2019-03-13T15:36:03.315239Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['purchase_amount', 'Cat_1_Y', 'Cat2Tot1', 'Cat2Tot2', 'Cat2Tot3',\n",
       "       'Cat2Tot4', 'Cat3TotA', 'Cat3TotB', 'AvgPurAmt', 'TimeSpent', 'CLV',\n",
       "       'numerical_2', 'avg_sales_lag3', 'avg_purchases_lag3',\n",
       "       'avg_purchases_lag6', 'active_months_lag6', 'avg_sales_lag12',\n",
       "       'feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5',\n",
       "       'feature_2_2', 'feature_2_3', 'feature_3_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T15:32:09.465338Z",
     "start_time": "2019-03-13T15:32:08.566069Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xtrain = pd.read_pickle('XtrainNN');Ytrain = pd.read_pickle('YtrainNN');Xtest = pd.read_pickle('XtestNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fitting NN on Elo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T15:33:25.183239Z",
     "start_time": "2019-03-12T15:33:25.167864Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:31:56.986065Z",
     "start_time": "2019-03-12T17:31:56.964818Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=24, activation='relu'))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:31:57.570465Z",
     "start_time": "2019-03-12T17:31:57.558329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:59:59.607607Z",
     "start_time": "2019-03-12T17:31:58.330601Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 10s 72us/step - loss: 2.8708 - acc: 0.0082\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8283 - acc: 0.0081\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8228 - acc: 0.0075\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8208 - acc: 0.0069\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8189 - acc: 0.0071\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8187 - acc: 0.0069\n",
      "Epoch 7/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8187 - acc: 0.0070\n",
      "Epoch 8/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8164 - acc: 0.0067\n",
      "Epoch 9/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8137 - acc: 0.0070\n",
      "Epoch 10/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8131 - acc: 0.0068\n",
      "Epoch 11/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8115 - acc: 0.0069\n",
      "Epoch 12/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8124 - acc: 0.0068\n",
      "Epoch 13/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.8093 - acc: 0.0066\n",
      "Epoch 14/100\n",
      "133070/133070 [==============================] - 11s 85us/step - loss: 2.8089 - acc: 0.0069\n",
      "Epoch 15/100\n",
      "133070/133070 [==============================] - 12s 87us/step - loss: 2.8080 - acc: 0.0071\n",
      "Epoch 16/100\n",
      "133070/133070 [==============================] - 9s 67us/step - loss: 2.8072 - acc: 0.0069\n",
      "Epoch 17/100\n",
      "133070/133070 [==============================] - 9s 67us/step - loss: 2.8061 - acc: 0.0068\n",
      "Epoch 18/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.8050 - acc: 0.0070\n",
      "Epoch 19/100\n",
      "133070/133070 [==============================] - 10s 74us/step - loss: 2.8045 - acc: 0.0070\n",
      "Epoch 20/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8034 - acc: 0.0071\n",
      "Epoch 21/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8053 - acc: 0.0067\n",
      "Epoch 22/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8037 - acc: 0.0069\n",
      "Epoch 23/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8010 - acc: 0.0068\n",
      "Epoch 24/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7989 - acc: 0.0072\n",
      "Epoch 25/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7982 - acc: 0.0067\n",
      "Epoch 26/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.7982 - acc: 0.0071\n",
      "Epoch 27/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7964 - acc: 0.0068\n",
      "Epoch 28/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.7984 - acc: 0.0069\n",
      "Epoch 29/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.7982 - acc: 0.0071\n",
      "Epoch 30/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.7940 - acc: 0.0069\n",
      "Epoch 31/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7927 - acc: 0.0068\n",
      "Epoch 32/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7903 - acc: 0.0067\n",
      "Epoch 33/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7905 - acc: 0.0069\n",
      "Epoch 34/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7891 - acc: 0.0069\n",
      "Epoch 35/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.7860 - acc: 0.0069\n",
      "Epoch 36/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7853 - acc: 0.0070\n",
      "Epoch 37/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7862 - acc: 0.0069\n",
      "Epoch 38/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7827 - acc: 0.0068\n",
      "Epoch 39/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7817 - acc: 0.0068\n",
      "Epoch 40/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7807 - acc: 0.0069\n",
      "Epoch 41/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7811 - acc: 0.0070\n",
      "Epoch 42/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7778 - acc: 0.0070\n",
      "Epoch 43/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7786 - acc: 0.0068\n",
      "Epoch 44/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7748 - acc: 0.0068\n",
      "Epoch 45/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7732 - acc: 0.0067\n",
      "Epoch 46/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7720 - acc: 0.0070\n",
      "Epoch 47/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7725 - acc: 0.0069\n",
      "Epoch 48/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7708 - acc: 0.0070\n",
      "Epoch 49/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7677 - acc: 0.0065\n",
      "Epoch 50/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7644 - acc: 0.0069\n",
      "Epoch 51/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7601 - acc: 0.0069\n",
      "Epoch 52/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.7669 - acc: 0.0070\n",
      "Epoch 53/100\n",
      "133070/133070 [==============================] - 13s 96us/step - loss: 2.7623 - acc: 0.0069\n",
      "Epoch 54/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.7572 - acc: 0.0066\n",
      "Epoch 55/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.7595 - acc: 0.0068\n",
      "Epoch 56/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7540 - acc: 0.0067\n",
      "Epoch 57/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7528 - acc: 0.0069\n",
      "Epoch 58/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7565 - acc: 0.0069\n",
      "Epoch 59/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7510 - acc: 0.0068\n",
      "Epoch 60/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7454 - acc: 0.0067\n",
      "Epoch 61/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7436 - acc: 0.0067\n",
      "Epoch 62/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7417 - acc: 0.0064\n",
      "Epoch 63/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7359 - acc: 0.0068\n",
      "Epoch 64/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7376 - acc: 0.0066\n",
      "Epoch 65/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7297 - acc: 0.0066\n",
      "Epoch 66/100\n",
      "133070/133070 [==============================] - 10s 75us/step - loss: 2.7283 - acc: 0.0066\n",
      "Epoch 67/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.7282 - acc: 0.0065\n",
      "Epoch 68/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7257 - acc: 0.0065\n",
      "Epoch 69/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.7266 - acc: 0.0067\n",
      "Epoch 70/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7237 - acc: 0.0067\n",
      "Epoch 71/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7153 - acc: 0.0065\n",
      "Epoch 72/100\n",
      "133070/133070 [==============================] - 10s 72us/step - loss: 2.7198 - acc: 0.0066\n",
      "Epoch 73/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7093 - acc: 0.0065\n",
      "Epoch 74/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7106 - acc: 0.0065\n",
      "Epoch 75/100\n",
      "133070/133070 [==============================] - 15s 113us/step - loss: 2.7070 - acc: 0.0063\n",
      "Epoch 76/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.7085 - acc: 0.0062\n",
      "Epoch 77/100\n",
      "133070/133070 [==============================] - 9s 66us/step - loss: 2.7015 - acc: 0.0064\n",
      "Epoch 78/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.7004 - acc: 0.0067\n",
      "Epoch 79/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.6987 - acc: 0.0064\n",
      "Epoch 80/100\n",
      "133070/133070 [==============================] - 10s 73us/step - loss: 2.6862 - acc: 0.0063\n",
      "Epoch 81/100\n",
      "133070/133070 [==============================] - 10s 77us/step - loss: 2.6826 - acc: 0.0065\n",
      "Epoch 82/100\n",
      "133070/133070 [==============================] - 11s 84us/step - loss: 2.6832 - acc: 0.0062\n",
      "Epoch 83/100\n",
      "133070/133070 [==============================] - 10s 77us/step - loss: 2.6734 - acc: 0.0063\n",
      "Epoch 84/100\n",
      "133070/133070 [==============================] - 8s 63us/step - loss: 2.6703 - acc: 0.0063\n",
      "Epoch 85/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6721 - acc: 0.0061\n",
      "Epoch 86/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6640 - acc: 0.0061\n",
      "Epoch 87/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6549 - acc: 0.0062\n",
      "Epoch 88/100\n",
      "133070/133070 [==============================] - 9s 68us/step - loss: 2.6518 - acc: 0.0062\n",
      "Epoch 89/100\n",
      "133070/133070 [==============================] - 10s 76us/step - loss: 2.6514 - acc: 0.0059\n",
      "Epoch 90/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6428 - acc: 0.0061\n",
      "Epoch 91/100\n",
      "133070/133070 [==============================] - 9s 69us/step - loss: 2.6503 - acc: 0.0061\n",
      "Epoch 92/100\n",
      "133070/133070 [==============================] - 8s 62us/step - loss: 2.6376 - acc: 0.0061\n",
      "Epoch 93/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6370 - acc: 0.0062\n",
      "Epoch 94/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6302 - acc: 0.0061\n",
      "Epoch 95/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6224 - acc: 0.0062\n",
      "Epoch 96/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.6174 - acc: 0.0060\n",
      "Epoch 97/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6173 - acc: 0.0060\n",
      "Epoch 98/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6139 - acc: 0.0060\n",
      "Epoch 99/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6056 - acc: 0.0061\n",
      "Epoch 100/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6090 - acc: 0.0060\n",
      "66535/66535 [==============================] - ETA:  - 2s 26us/step\n",
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.9220 - acc: 0.0080\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8705 - acc: 0.0081\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8663 - acc: 0.0081\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8634 - acc: 0.0080\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8611 - acc: 0.0075\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8588 - acc: 0.0071\n",
      "Epoch 7/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8579 - acc: 0.0071\n",
      "Epoch 8/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8556 - acc: 0.0068\n",
      "Epoch 9/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8545 - acc: 0.0067\n",
      "Epoch 10/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8550 - acc: 0.0067\n",
      "Epoch 11/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8532 - acc: 0.0066\n",
      "Epoch 12/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8514 - acc: 0.0066\n",
      "Epoch 13/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8511 - acc: 0.0066\n",
      "Epoch 14/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8491 - acc: 0.0066\n",
      "Epoch 15/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8469 - acc: 0.0067\n",
      "Epoch 16/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8476 - acc: 0.0066\n",
      "Epoch 17/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8474 - acc: 0.0067\n",
      "Epoch 18/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8454 - acc: 0.0065\n",
      "Epoch 19/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8437 - acc: 0.0066\n",
      "Epoch 20/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8423 - acc: 0.0069\n",
      "Epoch 21/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8448 - acc: 0.0068\n",
      "Epoch 22/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8416 - acc: 0.0068\n",
      "Epoch 23/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8390 - acc: 0.0067\n",
      "Epoch 24/100\n",
      "133070/133070 [==============================] - 9s 65us/step - loss: 2.8407 - acc: 0.0068\n",
      "Epoch 25/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8398 - acc: 0.0066\n",
      "Epoch 26/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8383 - acc: 0.0067\n",
      "Epoch 27/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8380 - acc: 0.0068\n",
      "Epoch 28/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8359 - acc: 0.0066\n",
      "Epoch 29/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8355 - acc: 0.0069\n",
      "Epoch 30/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8336 - acc: 0.0067\n",
      "Epoch 31/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8320 - acc: 0.0070\n",
      "Epoch 32/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8366 - acc: 0.0068\n",
      "Epoch 33/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8309 - acc: 0.0069\n",
      "Epoch 34/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8289 - acc: 0.0070\n",
      "Epoch 35/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8316 - acc: 0.0065\n",
      "Epoch 36/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8272 - acc: 0.0067\n",
      "Epoch 37/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8259 - acc: 0.0069\n",
      "Epoch 38/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.8233 - acc: 0.0067\n",
      "Epoch 39/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8221 - acc: 0.0067\n",
      "Epoch 40/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8225 - acc: 0.0068\n",
      "Epoch 41/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8191 - acc: 0.0066\n",
      "Epoch 42/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8182 - acc: 0.0064\n",
      "Epoch 43/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8195 - acc: 0.0069\n",
      "Epoch 44/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8185 - acc: 0.0066\n",
      "Epoch 45/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8156 - acc: 0.0068\n",
      "Epoch 46/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8116 - acc: 0.0066\n",
      "Epoch 47/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8119 - acc: 0.0067\n",
      "Epoch 48/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8106 - acc: 0.0067\n",
      "Epoch 49/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.8074 - acc: 0.0066\n",
      "Epoch 50/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8079 - acc: 0.0067\n",
      "Epoch 51/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8065 - acc: 0.0067\n",
      "Epoch 52/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.8038 - acc: 0.0066\n",
      "Epoch 53/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7989 - acc: 0.0067\n",
      "Epoch 54/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7962 - acc: 0.0065\n",
      "Epoch 55/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7936 - acc: 0.0064\n",
      "Epoch 56/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7913 - acc: 0.0065\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7944 - acc: 0.0068\n",
      "Epoch 58/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7874 - acc: 0.0062\n",
      "Epoch 59/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7820 - acc: 0.0066\n",
      "Epoch 60/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7862 - acc: 0.0066\n",
      "Epoch 61/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7897 - acc: 0.0063\n",
      "Epoch 62/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7846 - acc: 0.0064\n",
      "Epoch 63/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7754 - acc: 0.0066\n",
      "Epoch 64/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7744 - acc: 0.0064\n",
      "Epoch 65/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7734 - acc: 0.0061\n",
      "Epoch 66/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7699 - acc: 0.0063\n",
      "Epoch 67/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7670 - acc: 0.0063\n",
      "Epoch 68/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7681 - acc: 0.0063\n",
      "Epoch 69/100\n",
      "133070/133070 [==============================] - 8s 59us/step - loss: 2.7669 - acc: 0.0062\n",
      "Epoch 70/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7604 - acc: 0.0063\n",
      "Epoch 71/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.7535 - acc: 0.0062\n",
      "Epoch 72/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7547 - acc: 0.0064\n",
      "Epoch 73/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7473 - acc: 0.0061\n",
      "Epoch 74/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7468 - acc: 0.0062\n",
      "Epoch 75/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.7458 - acc: 0.0059\n",
      "Epoch 76/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.7344 - acc: 0.0062\n",
      "Epoch 77/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7303 - acc: 0.0059\n",
      "Epoch 78/100\n",
      "133070/133070 [==============================] - 7s 56us/step - loss: 2.7339 - acc: 0.0060\n",
      "Epoch 79/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7255 - acc: 0.0062\n",
      "Epoch 80/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7168 - acc: 0.0063\n",
      "Epoch 81/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7128 - acc: 0.0059\n",
      "Epoch 82/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7163 - acc: 0.0061\n",
      "Epoch 83/100\n",
      "133070/133070 [==============================] - 7s 56us/step - loss: 2.7053 - acc: 0.0061\n",
      "Epoch 84/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7038 - acc: 0.0061\n",
      "Epoch 85/100\n",
      "133070/133070 [==============================] - 8s 58us/step - loss: 2.6953 - acc: 0.0061\n",
      "Epoch 86/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.7009 - acc: 0.0060\n",
      "Epoch 87/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6938 - acc: 0.0059\n",
      "Epoch 88/100\n",
      "133070/133070 [==============================] - 8s 56us/step - loss: 2.6861 - acc: 0.0062\n",
      "Epoch 89/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6903 - acc: 0.0060\n",
      "Epoch 90/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6878 - acc: 0.0057\n",
      "Epoch 91/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6828 - acc: 0.0059\n",
      "Epoch 92/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6734 - acc: 0.0061\n",
      "Epoch 93/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6573 - acc: 0.0060\n",
      "Epoch 94/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6581 - acc: 0.0058\n",
      "Epoch 95/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6653 - acc: 0.0058\n",
      "Epoch 96/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6832 - acc: 0.0058\n",
      "Epoch 97/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6634 - acc: 0.0059\n",
      "Epoch 98/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6569 - acc: 0.0059\n",
      "Epoch 99/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6545 - acc: 0.0059\n",
      "Epoch 100/100\n",
      "133070/133070 [==============================] - 8s 57us/step - loss: 2.6425 - acc: 0.0058\n",
      "66535/66535 [==============================] - 2s 26us/step\n",
      "Epoch 1/100\n",
      "133070/133070 [==============================] - 9s 70us/step - loss: 2.9118 - acc: 0.0081\n",
      "Epoch 2/100\n",
      "133070/133070 [==============================] - 9s 64us/step - loss: 2.8506 - acc: 0.0082\n",
      "Epoch 3/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8446 - acc: 0.0081\n",
      "Epoch 4/100\n",
      "133070/133070 [==============================] - 9s 71us/step - loss: 2.8417 - acc: 0.0076\n",
      "Epoch 5/100\n",
      "133070/133070 [==============================] - 8s 61us/step - loss: 2.8403 - acc: 0.0074\n",
      "Epoch 6/100\n",
      "133070/133070 [==============================] - 8s 60us/step - loss: 2.8382 - acc: 0.0073\n",
      "Epoch 7/100\n",
      "100000/133070 [=====================>........] - ETA: 2s - loss: 2.8252 - acc: 0.0069"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-7288a762c50e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: %.2f (%.2f) MSE\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "results = cross_val_score(estimator, Xtrain, Ytrain, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Practice Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T17:02:54.180970Z",
     "start_time": "2019-03-12T17:02:54.149730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T17:16:51.328300Z",
     "start_time": "2019-03-13T17:09:20.544879Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "199605/199605 [==============================] - 11s 55us/step - loss: 192.0064 - acc: 0.0082\n",
      "Epoch 2/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 45.3411 - acc: 0.0082\n",
      "Epoch 3/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 22.4433 - acc: 0.0082\n",
      "Epoch 4/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 11.0558 - acc: 0.0082\n",
      "Epoch 5/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 7.2642 - acc: 0.0082\n",
      "Epoch 6/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 4.9516 - acc: 0.0082\n",
      "Epoch 7/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 3.9370 - acc: 0.0082\n",
      "Epoch 8/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 3.4658 - acc: 0.0082\n",
      "Epoch 9/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 3.2065 - acc: 0.0082\n",
      "Epoch 10/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 3.0911 - acc: 0.0082\n",
      "Epoch 11/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 3.0591 - acc: 0.0082\n",
      "Epoch 12/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 3.0359 - acc: 0.0082\n",
      "Epoch 13/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 3.0039 - acc: 0.0082\n",
      "Epoch 14/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.9675 - acc: 0.0082\n",
      "Epoch 15/50\n",
      "199605/199605 [==============================] - 9s 45us/step - loss: 2.9411 - acc: 0.0082\n",
      "Epoch 16/50\n",
      "199605/199605 [==============================] - 9s 47us/step - loss: 2.9329 - acc: 0.0082\n",
      "Epoch 17/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.9306 - acc: 0.0082\n",
      "Epoch 18/50\n",
      "199605/199605 [==============================] - 8s 41us/step - loss: 2.9251 - acc: 0.0082\n",
      "Epoch 19/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.9131 - acc: 0.0082\n",
      "Epoch 20/50\n",
      "199605/199605 [==============================] - 9s 47us/step - loss: 2.9018 - acc: 0.0082\n",
      "Epoch 21/50\n",
      "199605/199605 [==============================] - 9s 46us/step - loss: 2.8959 - acc: 0.0082\n",
      "Epoch 22/50\n",
      "199605/199605 [==============================] - 8s 41us/step - loss: 2.8933 - acc: 0.0082\n",
      "Epoch 23/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8911 - acc: 0.0082\n",
      "Epoch 24/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8877 - acc: 0.0082\n",
      "Epoch 25/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8833 - acc: 0.0082\n",
      "Epoch 26/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8791 - acc: 0.0082\n",
      "Epoch 27/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8763 - acc: 0.0082\n",
      "Epoch 28/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8749 - acc: 0.0082\n",
      "Epoch 29/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8743 - acc: 0.0082\n",
      "Epoch 30/50\n",
      "199605/199605 [==============================] - 8s 41us/step - loss: 2.8739 - acc: 0.0082\n",
      "Epoch 31/50\n",
      "199605/199605 [==============================] - 8s 43us/step - loss: 2.8735 - acc: 0.0082\n",
      "Epoch 32/50\n",
      "199605/199605 [==============================] - 9s 46us/step - loss: 2.8731 - acc: 0.0082\n",
      "Epoch 33/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8726 - acc: 0.0082\n",
      "Epoch 34/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8721 - acc: 0.0082\n",
      "Epoch 35/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8718 - acc: 0.0082\n",
      "Epoch 36/50\n",
      "199605/199605 [==============================] - 9s 45us/step - loss: 2.8715 - acc: 0.0082\n",
      "Epoch 37/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 2.8713 - acc: 0.0082\n",
      "Epoch 38/50\n",
      "199605/199605 [==============================] - 9s 46us/step - loss: 2.8710 - acc: 0.0082\n",
      "Epoch 39/50\n",
      "199605/199605 [==============================] - 9s 46us/step - loss: 2.8708 - acc: 0.0082\n",
      "Epoch 40/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8706 - acc: 0.0082\n",
      "Epoch 41/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 2.8704 - acc: 0.0082\n",
      "Epoch 42/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8703 - acc: 0.0082\n",
      "Epoch 43/50\n",
      "199605/199605 [==============================] - 9s 44us/step - loss: 2.8702 - acc: 0.0082\n",
      "Epoch 44/50\n",
      "199605/199605 [==============================] - 10s 49us/step - loss: 2.8701 - acc: 0.0082\n",
      "Epoch 45/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8700 - acc: 0.0082\n",
      "Epoch 46/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8700 - acc: 0.0082\n",
      "Epoch 47/50\n",
      "199605/199605 [==============================] - 9s 47us/step - loss: 2.8699 - acc: 0.0082\n",
      "Epoch 48/50\n",
      "199605/199605 [==============================] - 9s 45us/step - loss: 2.8699 - acc: 0.0082\n",
      "Epoch 49/50\n",
      "199605/199605 [==============================] - 8s 42us/step - loss: 2.8698 - acc: 0.0082\n",
      "Epoch 50/50\n",
      "199605/199605 [==============================] - 9s 43us/step - loss: 2.8698 - acc: 0.0082\n",
      "199605/199605 [==============================] - 14s 68us/step\n",
      "\n",
      "Loss: 2.87, Accuracy: 0.82%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=24, activation='relu'))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(75, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(30, activation='relu', activity_regularizer=l1(0.002)))\n",
    "model.add(Dense(1))\n",
    "# 2. compile the network\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['accuracy'])\n",
    "# 3. fit the network\n",
    "history = model.fit(Xtrain, Ytrain, epochs=50, batch_size=len(Xtrain))\n",
    "# 4. evaluate the network\n",
    "loss, accuracy = model.evaluate(Xtrain, Ytrain)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:27:33.145970Z",
     "start_time": "2019-03-13T03:27:26.350698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5. make predictions\n",
    "Ypred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:27:33.630235Z",
     "start_time": "2019-03-13T03:27:33.145970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "targets = pd.DataFrame(Ypred,columns=['target']); targets.to_csv('targets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T21:51:46.723081Z",
     "start_time": "2019-03-12T21:51:46.691836Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.077040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.146940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.440055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.958091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.107839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.687379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.412674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.264124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.073686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.511530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.445657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.645471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.434187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.060336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.224298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.223935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.245104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.106159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.051506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.611010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.312891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.130725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.812803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.349580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.467421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.304362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.016946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123593</th>\n",
       "      <td>-0.238736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123594</th>\n",
       "      <td>-0.064388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123595</th>\n",
       "      <td>0.139949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123596</th>\n",
       "      <td>-0.328311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123597</th>\n",
       "      <td>0.174461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123598</th>\n",
       "      <td>-0.097019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123599</th>\n",
       "      <td>-0.816208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123600</th>\n",
       "      <td>-0.773228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123601</th>\n",
       "      <td>0.045559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123602</th>\n",
       "      <td>-0.853169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123603</th>\n",
       "      <td>-0.325822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123604</th>\n",
       "      <td>-0.543833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123605</th>\n",
       "      <td>0.002259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123606</th>\n",
       "      <td>-0.535410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123607</th>\n",
       "      <td>0.727111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123608</th>\n",
       "      <td>-0.201082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123609</th>\n",
       "      <td>0.117222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123610</th>\n",
       "      <td>-0.800819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123611</th>\n",
       "      <td>0.240039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123612</th>\n",
       "      <td>-0.130933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123613</th>\n",
       "      <td>0.379224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123614</th>\n",
       "      <td>-1.597571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123615</th>\n",
       "      <td>-0.391914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123616</th>\n",
       "      <td>-0.026032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123617</th>\n",
       "      <td>-0.152784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123618</th>\n",
       "      <td>-0.148802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123619</th>\n",
       "      <td>0.341019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123620</th>\n",
       "      <td>-1.146644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123621</th>\n",
       "      <td>-0.660543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123622</th>\n",
       "      <td>-0.024831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123623 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "0       0.077040\n",
       "1       0.385990\n",
       "2       0.146940\n",
       "3      -0.440055\n",
       "4      -0.958091\n",
       "5      -0.107839\n",
       "6      -0.687379\n",
       "7       0.412674\n",
       "8       0.264124\n",
       "9       0.058995\n",
       "10     -0.002074\n",
       "11      0.073686\n",
       "12     -0.511530\n",
       "13     -0.445657\n",
       "14     -0.645471\n",
       "15     -0.434187\n",
       "16     -0.060336\n",
       "17      0.224298\n",
       "18      0.223935\n",
       "19      0.245104\n",
       "20      0.106159\n",
       "21     -0.051506\n",
       "22     -0.611010\n",
       "23      0.312891\n",
       "24      0.130725\n",
       "25      0.812803\n",
       "26     -0.349580\n",
       "27      0.467421\n",
       "28     -0.304362\n",
       "29      0.016946\n",
       "...          ...\n",
       "123593 -0.238736\n",
       "123594 -0.064388\n",
       "123595  0.139949\n",
       "123596 -0.328311\n",
       "123597  0.174461\n",
       "123598 -0.097019\n",
       "123599 -0.816208\n",
       "123600 -0.773228\n",
       "123601  0.045559\n",
       "123602 -0.853169\n",
       "123603 -0.325822\n",
       "123604 -0.543833\n",
       "123605  0.002259\n",
       "123606 -0.535410\n",
       "123607  0.727111\n",
       "123608 -0.201082\n",
       "123609  0.117222\n",
       "123610 -0.800819\n",
       "123611  0.240039\n",
       "123612 -0.130933\n",
       "123613  0.379224\n",
       "123614 -1.597571\n",
       "123615 -0.391914\n",
       "123616 -0.026032\n",
       "123617 -0.152784\n",
       "123618 -0.148802\n",
       "123619  0.341019\n",
       "123620 -1.146644\n",
       "123621 -0.660543\n",
       "123622 -0.024831\n",
       "\n",
       "[123623 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T18:54:53.256142Z",
     "start_time": "2019-03-11T18:54:48.015129Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = [float(round(x)) for x in probabilities]\n",
    "# accuracy = numpy.mean(predictions == Y)\n",
    "# print(\"Prediction Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Making feature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T15:37:23.916232Z",
     "start_time": "2019-03-13T15:37:23.853864Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xtrain2 = Xtrain[['feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1']]\n",
    "Xtest2 = Xtest[['feature_1_2', 'feature_1_3', 'feature_1_4', 'feature_1_5','feature_2_2', 'feature_2_3', 'feature_3_1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### NN on features only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T16:22:16.409351Z",
     "start_time": "2019-03-13T16:22:16.393698Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T15:48:02.873003Z",
     "start_time": "2019-03-13T15:46:47.143090Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "199605/199605 [==============================] - 2s 10us/step - loss: 2.8700 - acc: 0.0082\n",
      "Epoch 2/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8690 - acc: 0.0082\n",
      "Epoch 3/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8685 - acc: 0.0082\n",
      "Epoch 4/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8680 - acc: 0.0082\n",
      "Epoch 5/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 6/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 7/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8682 - acc: 0.0082\n",
      "Epoch 8/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8681 - acc: 0.0082\n",
      "Epoch 9/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 10/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 11/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 12/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8681 - acc: 0.0082\n",
      "Epoch 13/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 14/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 15/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 16/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 17/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 18/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 19/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 20/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 21/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 22/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8676 - acc: 0.0082\n",
      "Epoch 23/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 24/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 25/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 26/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 27/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 28/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 29/50\n",
      "199605/199605 [==============================] - 1s 5us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 30/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 31/50\n",
      "199605/199605 [==============================] - 2s 8us/step - loss: 2.8679 - acc: 0.0082A: 1s - loss: 2.8830 - ac\n",
      "Epoch 32/50\n",
      "199605/199605 [==============================] - 2s 10us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 33/50\n",
      "199605/199605 [==============================] - 2s 8us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 34/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8676 - acc: 0.0082\n",
      "Epoch 35/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 36/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 37/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 38/50\n",
      "199605/199605 [==============================] - 2s 8us/step - loss: 2.8681 - acc: 0.0082A: 1s - loss: 2.8258 -\n",
      "Epoch 39/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 40/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 41/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8680 - acc: 0.0082\n",
      "Epoch 42/50\n",
      "199605/199605 [==============================] - 2s 8us/step - loss: 2.8679 - acc: 0.0082\n",
      "Epoch 43/50\n",
      "199605/199605 [==============================] - 2s 9us/step - loss: 2.8677 - acc: 0.0082\n",
      "Epoch 44/50\n",
      "199605/199605 [==============================] - 2s 8us/step - loss: 2.8675 - acc: 0.0082\n",
      "Epoch 45/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8676 - acc: 0.0082\n",
      "Epoch 46/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8676 - acc: 0.0082\n",
      "Epoch 47/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8676 - acc: 0.0082\n",
      "Epoch 48/50\n",
      "199605/199605 [==============================] - 1s 7us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 49/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8678 - acc: 0.0082\n",
      "Epoch 50/50\n",
      "199605/199605 [==============================] - 1s 6us/step - loss: 2.8676 - acc: 0.0082\n",
      "199605/199605 [==============================] - 13s 63us/step\n",
      "\n",
      "Loss: 2.87, Accuracy: 0.82%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, input_dim=7, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# 2. compile the network\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['accuracy'])\n",
    "# 3. fit the network\n",
    "history = model.fit(Xtrain2, Ytrain, epochs=50, batch_size=15000)\n",
    "# 4. evaluate the network\n",
    "loss, accuracy = model.evaluate(Xtrain2, Ytrain)\n",
    "print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
